---
# nameOverride: rapids
# fullnameOverride: rapids

scheduler:
  name: scheduler # Dask scheduler name.
  enabled: true # Enable/disable scheduler.
  image:
    repository: "rapidsai/rapidsai-core" # Container image repository.
    tag: "22.06-cuda11.5-runtime-ubuntu20.04-py3.9" # Container image tag.
    pullPolicy: IfNotPresent # Container image pull policy.
    pullSecrets: # Container image [pull secrets](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).
    #  - name: regcred
  replicas: 1 # Number of schedulers (should always be 1).
  serviceType: "ClusterIP" # Scheduler service type. Set to `LoadBalancer` to expose outside of your cluster.
  # serviceType: "NodePort"
  # serviceType: "LoadBalancer"
  loadBalancerIP: null # Some cloud providers allow you to specify the loadBalancerIP when using the `LoadBalancer` service type. If your cloud does not support it this option will be ignored.
  servicePort: 8786 # Scheduler service internal port.
  serviceAnnotations: {} # Scheduler service annotations.
  extraArgs:
    [] # Extra CLI arguments to be passed to the scheduler
    # - --preload
    # - scheduler-setup.py
  resources: {} # Scheduler pod resources. See `values.yaml` for example values.
  #  limits:
  #    cpu: 1.8
  #    memory: 6G
  #  requests:
  #    cpu: 1.8
  #    memory: 6G
  tolerations: [] # Tolerations.
  affinity: {} # Container affinity.
  nodeSelector: {} # Node Selector.
  securityContext: {} # Security Context.
  # serviceAccountName: ""
  env:
  - name: DISABLE_JUPYTER
    value: "true"
  metrics:
    enabled: false # Enable scheduler metrics. Pip package [prometheus-client](https://pypi.org/project/prometheus-client/) should be present on scheduler.
    serviceMonitor:
      enabled: false # Enable scheduler servicemonitor.
      namespace: "" # Deploy servicemonitor in different namespace, e.g. monitoring.
      namespaceSelector: {} # Selector to select which namespaces the Endpoints objects are discovered from.
      # Default: scrape .Release.Namespace only
      # To scrape all, use the following:
      # namespaceSelector:
      #   any: true
      additionalLabels: {} # Additional labels to add to the ServiceMonitor metadata.
      interval: 30s # Interval at which metrics should be scraped.
      jobLabel: "" # The label to use to retrieve the job name from.
      targetLabels: [] # TargetLabels transfers labels on the Kubernetes Service onto the target.
      metricRelabelings: [] # MetricRelabelConfigs to apply to samples before ingestion.

webUI:
  name: webui # Dask webui name.
  servicePort: 80 # webui service internal port.

worker:
  name: worker # Dask worker name.
  image:
    repository: "rapidsai/rapidsai-core" # Container image repository.
    tag: "22.06-cuda11.5-runtime-ubuntu20.04-py3.9" # Container image tag.
    pullPolicy: IfNotPresent # Container image pull policy.
    dask_worker: "dask-cuda-worker" # Dask worker command. Using `dask-cuda-worker` for GPU worker.
    pullSecrets: # Container image [pull secrets](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).
    #  - name: regcred
  replicas: 3 # Number of workers.
  strategy:
    type: RollingUpdate # Strategy used to replace old Pods with new ones.
  custom_scheduler_url: null # connect to already existing scheduler, deployed not by this chart.
  env: # Environment variables. See `values.yaml` for example values.
  - name: DISABLE_JUPYTER
    value: "true"
  #  - name: EXTRA_APT_PACKAGES
  #    value: build-essential openssl
  #  - name: EXTRA_CONDA_PACKAGES
  #    value: numba xarray -c conda-forge
  #  - name: EXTRA_PIP_PACKAGES
  #    value: s3fs dask-ml prometheus-client --upgrade
  extraArgs:
    [] # Extra CLI arguments to be passed to the worker
    # - --preload
    # - worker-setup.py
  resources: # Worker pod resources. See `values.yaml` for example values.
   limits:
     cpu: 1
     memory: 3G
     nvidia.com/gpu: 1
   requests:
     cpu: 1
     memory: 3G
     nvidia.com/gpu: 1
  mounts: {} # Worker Pod volumes and volume mounts, mounts.volumes follows kuberentes api v1 Volumes spec. mounts.volumeMounts follows kubernetesapi v1 VolumeMount spec
  #  volumes:
  #    - name: data
  #      emptyDir: {}
  #  volumeMounts:
  #    - name: data
  #      mountPath: /data
  annotations: {} # Annotations
  tolerations: [] # Tolerations.
  affinity: {} # Container affinity.
  nodeSelector: {} # Node Selector.
  securityContext: {} # Security Context.
  # serviceAccountName: ""
  # port: ""
  portDashboard: 8790 # Worker dashboard and metrics port.
  #  this option overrides "--nthreads" on workers, which defaults to resources.limits.cpu / default_resources.limits.cpu
  #  use it if you need to limit the amount of threads used by multicore workers, or to make workers with non-whole-number cpu limits
  # threads_per_worker: 1

jupyter:
  name: jupyter # Jupyter name.
  enabled: true # Enable/disable the bundled Jupyter notebook.
  rbac: true # Create RBAC service account and role to allow Jupyter pod to scale worker pods and access logs.
  image:
    repository: "rapidsai/rapidsai-core" # Container image repository.
    tag: "22.06-cuda11.5-runtime-ubuntu20.04-py3.9" # Container image tag.
    pullPolicy: IfNotPresent # Container image pull policy.
    pullSecrets: # Container image [pull secrets](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).
    #  - name: regcred
    #
  replicas: 1 # Number of notebook servers.
  serviceType: "ClusterIP" # Scheduler service type. Set to `LoadBalancer` to expose outside of your cluster.
  # serviceType: "NodePort"
  # serviceType: "LoadBalancer"
  servicePort: 80 # Jupyter service internal port.
  # This hash corresponds to the password 'dask'
  password: "argon2:$argon2id$v=19$m=10240,t=10,p=8$vjAd2nixwgznJom1OGrEUQ$kDEY1c7uMR2/Clu6FiHQ1EdMnY6uY2jgprvC0hSx2NU" # Password hash. Default hash corresponds to the password `dask`.
  env: # Environment variables. See `values.yaml` for example values.
  #  - name: EXTRA_CONDA_PACKAGES
  #    value: "numba xarray -c conda-forge"
  #  - name: EXTRA_PIP_PACKAGES
  #    value: "s3fs dask-ml --upgrade"
  command: null # Container command.
  args: # Container arguments.
  #  - "start.sh"
  #  - "jupyter"
  #  - "lab"
  extraConfig: |-
    # Extra Jupyter config goes here
    # E.g
    # c.NotebookApp.port = 8888
  resources: {} # Jupyter pod resources. See `values.yaml` for example values.
  #  limits:
  #    cpu: 2
  #    memory: 6G
  #  requests:
  #    cpu: 2
  #    memory: 6G
  mounts: {} # Worker Pod volumes and volume mounts, mounts.volumes follows kuberentes api v1 Volumes spec. mounts.volumeMounts follows kubernetesapi v1 VolumeMount spec
  #  volumes:
  #    - name: data
  #      emptyDir: {}
  #  volumeMounts:
  #    - name: data
  #      mountPath: /data
  tolerations: [] # Tolerations.
  affinity: {} # Container affinity.
  nodeSelector: {} # Node Selector.
  securityContext: {} # Security Context.
  serviceAccountName: "dask-jupyter" # Service account for use with RBAC
